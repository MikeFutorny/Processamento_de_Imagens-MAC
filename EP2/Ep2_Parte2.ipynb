{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "### EP2 MAC0417 / MAC5768\n",
        "##################################################################\n",
        "# AO PREENCHER ESSE CABEÇALHO COM O MEU NOME E O MEU NÚMERO USP,#\n",
        "# DECLARO QUE SOU O ÚNICO AUTOR E RESPONSÁVEL PELA RESOLUÇÃO #\n",
        "# DESTE EP. #\n",
        "# TODAS AS PARTES FORAM DESENVOLVIDAS E IMPLEMENTADAS POR MIM, #\n",
        "# SEGUINDO AS INSTRUÇÕES E QUE PORTANTO, NÃO CONSTITUEM #\n",
        "# DESONESTIDADE ACADÊMICA OU PLÁGIO. #\n",
        "# #\n",
        "# DECLARO TAMBÉM, QUE SOU RESPONSÁVEL POR TODAS AS CÓPIAS #\n",
        "# DESSE PROGRAMA, E QUE EU NÃO DISTRIBUI OU FACILITEI A #\n",
        "# SUA DISTRIBUIÇÃO. ESTOU CIENTE QUE OS CASOS DE PLÁGIO E #\n",
        "# DESONESTIDADE ACADÊMICA SERÃO TRATADOS SEGUNDO OS CRITÉRIOS #\n",
        "# DEFINIDOS NO CÓDIGO DE ÉTICA DA USP. #\n",
        "# #\n",
        "# ENTENDO QUE JUPYTER NOTEBOOKS SEM ASSINATURA NÃO SERÃO #\n",
        "# CORRIGIDOS E, AINDA ASSIM, PODERÃO SER PUNIDOS POR #\n",
        "# DESONESTIDADE ACADÊMICA. #\n",
        "# #\n",
        "# #\n",
        "# Nome : # Mikhail Futorny\n",
        "# NUSP : # 5258765\n",
        "# Turma: # MAC0417\n",
        "# Prof.: # Ronaldo Fumio Hashimoto\n",
        "##################################################################"
      ],
      "metadata": {
        "id": "uN_XQ7AlZ0iQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7vCrAKatC7Ij"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_dataset_folder = './content/augmentedDataSet'\n",
        "metadata_path = './content/Metadados/metadados.csv'\n",
        "normalized_dataset_folder = './content/normalizedDataset'\n",
        "\n",
        "metadata = pd.read_csv(metadata_path) #Leitura dos metadados\n",
        "if not os.path.exists(normalized_dataset_folder):\n",
        "    os.makedirs(normalized_dataset_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "Ybwka5P9Mr8r",
        "outputId": "f08c6d44-333c-41d5-e6bb-93cd42a02b81"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e724c72f9473>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnormalized_dataset_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/normalizedDataset'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Leitura dos metadados\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def histogram_equalization(image):\n",
        "    hist, bins = np.histogram(image.flatten(), bins=256, range=(0, 255), density=True)\n",
        "\n",
        "    acumulada = hist.cumsum()\n",
        "    acumulada_normalizada = (acumulada - acumulada.min()) / (acumulada.max() - acumulada.min())  # Normalização da acumulada\n",
        "\n",
        "    # Equalização\n",
        "    equalized_image = np.interp(image.flatten(), bins[:-1], acumulada_normalizada)\n",
        "    equalized_image = equalized_image.reshape(image.shape)\n",
        "\n",
        "    return equalized_image"
      ],
      "metadata": {
        "id": "COC-XTsbNART"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_histogram(image, title):\n",
        "    plt.hist(image.ravel(), bins=256, range=(0, 1), alpha=0.7, color='b', density=True)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Intensidade')\n",
        "    plt.ylabel('Frequencia normalizada')\n",
        "    plt.xlim([0, 1])\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "2gK65IAMNQRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_from_folder(folder):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder):\n",
        "        img_path = os.path.join(folder, filename)\n",
        "        img = plt.imread(img_path)\n",
        "        if img is not None:\n",
        "            images.append((filename, img))\n",
        "    return images"
      ],
      "metadata": {
        "id": "w7C2z6O8LZi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_datasets(dataset_folder):\n",
        "    mean_histograms_before = []\n",
        "    mean_histograms_after = []\n",
        "    images_before_after = []\n",
        "\n",
        "    for filename in os.listdir(dataset_folder): #Fazemos equalizacao para cada imagem no dataset\n",
        "        img_path = os.path.join(dataset_folder, filename)\n",
        "        img = plt.imread(img_path)\n",
        "\n",
        "\n",
        "        img_equalized = histogram_equalization(img)\n",
        "\n",
        "        hist_before, _ = np.histogram(img.flatten(), bins=256, range=(0,256))\n",
        "        hist_after, _ = np.histogram(img_equalized.flatten(), bins=256, range=(0,256))\n",
        "\n",
        "        mean_histograms_before.append(hist_before)\n",
        "        mean_histograms_after.append(hist_after)\n",
        "\n",
        "        images_before_after.append((img, img_equalized))\n",
        "\n",
        "    mean_hist_before = np.mean(mean_histograms_before, axis=0)\n",
        "    mean_hist_after = np.mean(mean_histograms_after, axis=0)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(mean_hist_before)\n",
        "    plt.title('Histograma médio antes da normalização')\n",
        "    plt.xlabel('Intensidade Pixel')\n",
        "    plt.ylabel('Frequencia')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(mean_hist_after)\n",
        "    plt.title('Histograma médio depois da normalização')\n",
        "    plt.xlabel('Intensidade Pixel')\n",
        "    plt.ylabel('Frequencia')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    num_samples = min(len(images_before_after), 5)  # Mostramos 5 imagens de exemplo\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    for i in range(num_samples):\n",
        "        plt.subplot(2, num_samples, i + 1)\n",
        "        plt.imshow(images_before_after[i][0], cmap='gray')\n",
        "        plt.title('Antes')\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(2, num_samples, i + num_samples + 1)\n",
        "        plt.imshow(images_before_after[i][1], cmap='gray')\n",
        "        plt.title('Depois')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2giICBKDNkQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(normalized_dataset_folder):\n",
        "    os.makedirs(normalized_dataset_folder)\n",
        "\n",
        "# Apply histogram equalization and save normalized images\n",
        "for subdir in os.listdir(augmented_dataset_folder):\n",
        "    subdir_path = os.path.join(augmented_dataset_folder, subdir)\n",
        "    if os.path.isdir(subdir_path):\n",
        "        normalized_subdir_path = os.path.join(normalized_dataset_folder, subdir)\n",
        "        if not os.path.exists(normalized_subdir_path):\n",
        "            os.makedirs(normalized_subdir_path)\n",
        "\n",
        "        for filename in os.listdir(subdir_path):\n",
        "            if filename.endswith('.jpeg'):\n",
        "                img_path = os.path.join(subdir_path, filename)\n",
        "                img = io.imread(img_path, as_gray=True)\n",
        "\n",
        "                # Apply histogram equalization\n",
        "                img_equalized = histogram_equalization(img)\n",
        "\n",
        "                # Save normalized image\n",
        "                save_path = os.path.join(normalized_subdir_path, filename)\n",
        "                io.imsave(save_path, img_equalized)"
      ],
      "metadata": {
        "id": "nGQ-NNKXNqQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Normal:')\n",
        "analyze_datasets('./content/augmentedDataSet/Normal')\n",
        "print('Contraste:')\n",
        "analyze_datasets('./content/augmentedDataSet/Contraste')\n",
        "print('Exponencial:')\n",
        "analyze_datasets('./content/augmentedDataSet/Exponencial')\n",
        "print('Laplaciano:')\n",
        "analyze_datasets('./content/augmentedDataSet/Laplaciano')\n",
        "print('MeanFilter:')\n",
        "analyze_datasets('./content/augmentedDataSet/MeanFilter')\n",
        "print('Logaritmo:')\n",
        "analyze_datasets('./content/augmentedDataSet/Logaritmo')"
      ],
      "metadata": {
        "id": "wZsYO80IQ-fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusão: O histograma médio após normalização jogou muito peso para os pixeis de valor zero, inicialmente estranho. Entretanto, dado o caráter preto e branco da imagem é um comportamento válido possível. Em relação às transformações, o augmented dataset no geral melhorou o contraste entre os objetos, sendo os de logaritmo e exponencial os melhores, provavelmente graças à escolha apropriada dos parâmetros, que é adaptada ao escuro, o que resulta em uma ótima performance para o dataset das imagens escuras, em relação às outras transformações. Em relação à normalização, o impacto mais significativo foi para o Laplaciano, que gerou a melhor visibilidade após a normalização. Para as outras transformações também foi positivo, porém com menor impacto. Percebe-se que seu impacto é bom tanto em casos escuros e claros. Em suma, podemos concluir que as transformações implementadas tem impacto positivo na visualização do Dataset."
      ],
      "metadata": {
        "id": "V6CW3AlyZogv"
      }
    }
  ]
}